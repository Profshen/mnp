{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup  \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####@author:gargla\n",
    "\n",
    "def cleanup_text(text):\n",
    "    text = str(text).encode('utf-8').decode('ascii', 'ignore')\n",
    "    step1Cleaned = BeautifulSoup(text,\"lxml\")\n",
    "    import re\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      step1Cleaned.get_text() )  # The text to search\n",
    "    lower_case = letters_only.lower()        \n",
    "    words = lower_case.split()\n",
    "    words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "    return words\n",
    "\n",
    "##get LIWC features for given word tokens\n",
    "def get_LIWC_features(words,categories):\n",
    "    feature_vector={}\n",
    "    for key,val in categories.items(): \n",
    "        itemcounter = {item:count for item, count in [(item, words.count(item)) for item in set(val)]}\n",
    "        feature_vector[key]=sum(itemcounter.values())\n",
    "    return feature_vector\n",
    "\n",
    "def get_data_feature_vector(ResponseDataset):\n",
    "    columns = ResponseDataset.columns.values\n",
    "    categoryFileMap=read_categories_fileMap()\n",
    "    categories = read_LIWC_categories(categoryFileMap)\n",
    "    #print columns\n",
    "    count = 0\n",
    "    feature_vector = {}\n",
    "    column_ignore_list=[\"id\",\"batch\"]\n",
    "    ##TODO:add exception handling insted\n",
    "    user_ignore_list=[\"usmt12\",\"usmt34\",\"nan\",\"usmt37\",\"ukbr1\",\n",
    "                      \"ukbr7\",\"ukbr25\",\"ukbr41\",\"ukbr73\",\"us12\",\"us15\",\n",
    "                      \"us39\",\"p3\",\"p9\",\"p18\",\"p35\",\"p41\",\"p46\",\"p53\",\"p62\",\n",
    "                      \"p69\",\"p72\",\"p76\",\"p91\",\"j6\",\"j9\",\"j10\",\"j17\",\"j25\",\n",
    "                      \"j26\",\"j30\",\"j36\",\"j42\",\"j43\",\"j45\",\"j75\",\"j90\",\"j111\",\n",
    "                     \"j113\",\"j115\",\"j117\",\"j118\",\"j120\",\"j124\",\"j126\",\"j127\",\"j128\",\"j130\", 'R_1r2C2j6mQ6GiA9k','R_24JjTIBz78Oijl9','R_2aWh2V161nCrCGr','R_cCQ86Yfo950NHzj','R_1PXaNHi9JlURf3b','R_2B8tle2pNhMhrGU','R_3Jltjep2enqKutm','R_1gqIhSkbGH6PRBh','R_3hgUJxvEwwLBr86','R_02LUmCakVkQk9IB','R_1mqvgCSXg8Lxpk2','R_0eKd3rQ9sS4lymt','R_1pX6VSWvUZE8ZhG','R_3kpMaZ2XOUd0xd3','R_231ypXR5P2cIrCq','R_3kuZQv8k2bd9RYC','R_UngyCbt3wGbXPH3','R_AKy9v28nGXPOS9b','R_3m9QTYqCAUFvgKu','R_2abEP94Q6Tbx4kz','R_2zpamsZRsB9r8Q3','R_3nNQ7DXcxNlKJZj','R_2xIRrbvLFp6Rgh0','R_0fGdxXrfIre7itX','R_2VPUUhhERZejlqm','R_3MxuYPQUGjZjzBe','R_2dlMNO2vIJcLpc0','R_1etx1ZroxGgJ0Nv','R_1jVq9aNBh5zCjzQ','R_3kogi5J6i5sYoW6','R_1CdLcMeiGsi4jSt','R_3iK0ImwTmH7s4rL','R_1H658H1dIDY29ds','R_emws4iZFRlrCduV','R_u9tgEqxhepGrplL','R_3PvLHfBO1gZF7as','R_doopIPuO5xvwnVn','R_UAypiJYGI5f5wjf','R_Qn8KsYwgFO6dSHn','R_3MDJomYB4kkhX0H','R_3JfuTdDybtscIvl','R_21H5aQMAsaG7u7a','R_1mqF3wysSzLaG8Z','R_ROcZUZiNGHaKSM9','R_AF36i1gXjpeQaAh','R_2rA5pwmIh4aILzl','R_3g0FGM0DQ9LVdHJ']\n",
    "    for row in range(ResponseDataset.shape[0]):\n",
    "    #iterate over each row to get categories for every user response and followups\n",
    "        userfeatureDict = {}\n",
    "        user = str(ResponseDataset[\"id\"][row])\n",
    "        if user in user_ignore_list:\n",
    "            continue\n",
    "        print \"getting features for id:\",user\n",
    "        for column in columns.tolist():\n",
    "            if column in column_ignore_list:\n",
    "                continue\n",
    "            text = ResponseDataset[column][row]\n",
    "            words = cleanup_text(text)\n",
    "            #print words\n",
    "            features = get_LIWC_features(words,categories)\n",
    "            features['wordCount']=len(words)\n",
    "            #print column,\":\",features\n",
    "            userfeatureDict[str(column)]=features\n",
    "        feature_vector[user] = userfeatureDict\n",
    "    return feature_vector\n",
    "\n",
    "#pd.DataFrame(featureDict.items(), columns=[\"text\",\"featurecount\"])\n",
    "    \n",
    "def create_feature_df(feature_matrix):\n",
    "    user_ids = []\n",
    "    frames = []\n",
    "    for user_id, d in feature_matrix.iteritems():\n",
    "        user_ids.append(user_id)\n",
    "        #user_df=pd.DataFrame.from_dict(d, orient='index').stack().reset_index()\n",
    "        user_df=json_normalize(d)\n",
    "        frames.append(user_df)\n",
    "    \n",
    "    feature_df = pd.concat(frames, keys=user_ids)\n",
    "    feature_df['id']=list(feature_df.index.levels[0])\n",
    "    return feature_df\n",
    "\n",
    "##we have categories as number.csv\n",
    "###We have a map that translates numbers to the category\n",
    "def read_categories_fileMap():\n",
    "    catKeyDict = {}\n",
    "    with open(\"LIWCDictKey.csv\", 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            key,val= line.strip().split(',')\n",
    "            catKeyDict[int(key)]=val\n",
    "    return catKeyDict\n",
    "\n",
    "\n",
    "def read_LIWC_categories(catKeyDict):\n",
    "    ##TODO:move path to func param\n",
    "    path=\"english_dictionary\"\n",
    "    csvs = [f for f in listdir(path) if f.endswith('.csv')]\n",
    "    categories={}\n",
    "    for csv in csvs:\n",
    "        CatList=[]\n",
    "        with open(path+\"/\"+csv, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                l= line.strip().split(',')\n",
    "                CatList.extend(l)\n",
    "                category,extension=csv.strip().split('.')\n",
    "        ##get the actual category from number.csv        \n",
    "        categories[catKeyDict[int(category)]]=CatList\n",
    "    return categories\n",
    "\n",
    "\n",
    "def processResponses(responsesFilePath):\n",
    "    ResponseDf = pd.ExcelFile(responsesFilePath)\n",
    "    ResponseDataset = ResponseDf.parse(0)\n",
    "    feature_vector = get_data_feature_vector(ResponseDataset)\n",
    "    feature_vector_df = create_feature_df(feature_vector)\n",
    "    return feature_vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encodeBinary(x, threshold=4):\n",
    "    \n",
    "    \"\"\"\n",
    "    encodes value into a binary variable i.e. if value > threshold: return 1, else return 0\n",
    "    Using this function to encode the entire Personality Dataframe\n",
    "    \n",
    "    Arguments: value, threshold(optional)<default=5>\n",
    "    \n",
    "    Output: binary value- 0 or 1\n",
    "    \n",
    "    \"\"\"\n",
    "    if(x>threshold):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encodeDF(personalityFilePath):\n",
    "    \"\"\"\n",
    "    encodes the personality score data into binary labels.  \n",
    "    \n",
    "    Arguments: path for the Personality Score csv file\n",
    "    \n",
    "    Output: Data frame with binary labels against each big 5 personality trait for each user\n",
    "    \n",
    "    \"\"\"\n",
    "    persData=pd.read_csv(personalityFilePath, header=0)\n",
    "    persData=persData.iloc[:,[0, 131,132,133,134,135]]\n",
    "    persData=persData[persData.Openness!=\"#NULL!\"] #Removing rows with no Personality form responses\n",
    "    persData[persData.columns[1:]]=persData[persData.columns[1:]].apply(pd.to_numeric)\n",
    "    thresholds=persData.quantile(0.7)\n",
    "    thresholds2=persData.quantile(0.3)\n",
    "    print(thresholds)\n",
    "    cols=persData.columns.values[1:]\n",
    "#     print persData[[cols[0]]].head()\n",
    "    \n",
    "    for i,col in enumerate(cols):\n",
    "        persData.loc[persData[col]<=thresholds2[i],[col]]=0\n",
    "        persData.loc[persData[col]>=thresholds[i],[col]]=1\n",
    "#             persData[i]=persData[i].map(encodeBinary)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return persData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "responsesFilePath=\"FINALNEWDATA.xlsx\"\n",
    "personalityFilePath=\"persData.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "liwcData=processResponses(responsesFilePath)\n",
    "persData=encodeDF(personalityFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print liwcData.head(10)\n",
    "print persData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merging LIWC and Personality data to create final dataset\n",
    "\n",
    "finalData=pd.merge(liwcData,persData, left_on='id', right_on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print finalData.tail(10)\n",
    "finalData.to_csv(\"finalDataWithSW.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
